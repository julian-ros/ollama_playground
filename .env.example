# Model Configuration Example
# Copy this file to .env and modify as needed

# Streamlit Frontend Chat Model
# Popular options: llama3.2, llama3.1, llama2, mistral, codellama
OLLAMA_CHAT_MODEL_STREAMLIT=llama3.2

# Embeddings API Chat Model (for document-enhanced responses)
# Should be a good general-purpose model
OLLAMA_CHAT_MODEL_EMBEDDINGS_API=llama2

# Embeddings Model (for document vectorization)
# Options: all-minilm, nomic-embed-text, mxbai-embed-large
OLLAMA_EMBEDDINGS_MODEL=all-minilm

# Ollama Base URLs (usually don't need to change these)
OLLAMA_BASE_URL_CHAT=http://ollama-chat:11434
OLLAMA_BASE_URL_EMBEDDINGS=http://ollama-embeddings:11434

# API Configuration
EMBEDDINGS_API_URL=http://embeddings-api:8000
EMBEDDINGS_DATA_PATH=/app/data